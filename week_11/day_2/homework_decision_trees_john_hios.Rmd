---
title: "Decision trees homework"
author: "John Hios"
date: "22/05/2022"
output:
  html_document:
    keep_md: yes
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    theme: cerulean #flatly #journal #cosmo
  pdf_document: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
```


<br>
In this homework we will create a decision tree to see which factors are useful in predicting whether or not a passenger on the titanic will survive.  


Run the code below before you begin: 


```{r, warning = FALSE, message = FALSE}
library(rpart)
library(rpart.plot)
library(tidyverse)

library(tidyverse)
titanic_set <- read_csv('data/titanic_decision_tree_data.csv')

shuffle_index <- sample(1:nrow(titanic_set))

# shuffle the data so class order isn't in order - need this for training/testing split later on 
titanic_set <- titanic_set[shuffle_index, ]
```

<br>

**Data Dictionary**

  * **sex**: Biological Sex, male or female  
  * **age_status**: adult or child (child defined as under 16)  
  * **class** : Ticket class, 1 = 1st (Upper class), 2 = 2nd (Middle Class), 3 = 3rd (Lower Class)    
  * **port_embarkation**: C = Cherbourg, Q = Queenstown, S = Southampton  
  * **sibsp** : number of siblings / spouses aboard the Titanic   
  * **parch**: number of parents / children aboard the Titanic. Some children travelled only with a nanny, therefore parch=0 for them. 
  * **survived_flag** : did they survive, 0 = No, 1 = Yes  



# MVP 


## Question 1  

<br> 
Cleaning up the data is always the first step. Do the following: 

  * Take only observations which have a `survived` flag (i.e. that aren't missing)  
  * Turn your important variables into factors (sex, survived, pclass, embarkation)  
  * Create an `age_status` variable which groups individuals under (and including) 16 years of age into a category called "child" category and those over 16 into a category called "adult".  
  * Drop the NA  
  * Drop any variables you don't need (`X1`, `passenger_id`, `name`, `ticket`, `far`, `cabin`)  

<br>

**Ans**

View data:
```{r}
titanic_set %>%
  glimpse()
```
Check for NA values:
```{r}
titanic_set %>% 
  summarise(across(.cols = everything(),
                   .fns = ~sum(is.na(.x))))

```

Clean-up data (if you follow blindly the instructions above, the resulting dataset will have very few rows (about 183) because drop_na() is not called at the end of the code chunk):
```{r}

titanic_clean <- titanic_set %>% 
  # remove observations which have a ``survived`` flag equal to NA
  filter(!is.na(survived))  %>% 
  # create an ``age_status`` variable and factor important variables  
  mutate(
    sex = as.factor(sex), 
    survived = factor(survived, levels = c(0,1), labels = c("No", "Yes")), 
    pclass = factor(pclass, levels = c(1, 2, 3), labels = c("Upper", "Middle", "Lower")), 
    embarked = as.factor(embarked),
    age_status = as.factor(if_else(age <= 16, "child", "adult"))
  ) %>% 
  # keep useful variables
  select(sex, survived, pclass, embarked, age_status, sib_sp, parch) %>% 
  # drop NA values
  drop_na()

glimpse(titanic_clean)
```

Check for NA values:
```{r}
titanic_clean %>% 
  summarise(across(.cols = everything(),
                   .fns = ~sum(is.na(.x))))

```


<br>

## Question 2  

<br> 
Have a look at your data and create some plots to ensure you know what you're working with before you begin. Write a summary of what you have found in your plots. Which variables do you think might be useful to predict whether or not people are going to die? Knowing this before you start is the best way to have a sanity check that your model is doing a good job.  

<br>
**Ans**
To find relationships between variables, split data in numeric and non-numeric
```{r}
titanic_clean_numeric <- titanic_clean %>%
  select_if(is.numeric)

titanic_clean_numeric$survived <- titanic_clean$survived



titanic_clean_nonnumeric <- titanic_clean %>%
  select_if(function(x) !is.numeric(x))
```


View correlations 
```{r}
library(GGally)

titanic_clean_numeric %>%
  ggpairs() 
```

```{r}
titanic_clean_nonnumeric %>%
  ggpairs()
```

Check if ``parch`` is worth including in variables useful for prediction:
```{r}
titanic_clean_numeric %>% 
  ggplot() +
  aes(x = parch, fill = survived) +
  geom_histogram(position = "dodge") 
  
```

Variables that are likely to be useful for prediction:

* ``parch`` (the histogram distribution with ``survived`` variable  shows some variation)

* ``embrarked`` (the bar plots with ``survived`` show some variation)

* ``pclass`` (the bar plots with ``survived`` show some variation)

* ``sex`` (the bar plots with ``survived`` show some variation)

* ``age_status`` (the bar plots with ``survived`` show some variation)


## Question 3  

<br> 
Now you can start to build your model. Create your testing and training set using an appropriate split. Check you have balanced sets. Write down why you chose the split you did and produce output tables to show whether or not it is balanced. [**Extra** - if you want to force balanced testing and training sets, have a look at the `stratified()` function in package `splitstackshape` (you can specify multiple variables to stratify on by passing a vector of variable names to the `group` argument, and get back testing and training sets with argument `bothSets = TRUE`)]

**Ans**


```{r}
# set the random seed number
set.seed(45678)
```


Case#1: 80-20 split without stratification:
```{r}
n_data <- nrow(titanic_clean)

# create a test sample index
test_index <- sample(1:n_data, size = n_data*0.2)

# create test set
titanic_test  <- slice(titanic_clean, test_index)

# create training set
titanic_train <- slice(titanic_clean, -test_index)
```

```{r}
# check balanced sets
titanic_test %>%
 janitor::tabyl(survived)
```

```{r}
titanic_train %>%
  janitor::tabyl(survived)
```

Case#2: 80-20 split with stratification per the variables shown below (I didn't stratify per ``parch`` because there weren't enough rows for all distinct groups):
```{r}
library(splitstackshape)

sets <- stratified(titanic_clean, group = c("survived", "embarked", "pclass", "sex", "age_status"), bothSets = TRUE, size = 0.2)

titanic_test_strat <- sets$SAMP1
titanic_train_strat <- sets$SAMP2


tibble(
  nrows_testing_strat = nrow(titanic_test_strat),
  nrows_training_strat = nrow(titanic_train)
)
```

Check percentages of stratified variables (testing and training)

1. Variable ``survived``
```{r}
titanic_test_strat %>%
 janitor::tabyl(survived)
```

```{r}
titanic_train_strat %>%
 janitor::tabyl(survived)
```

2. Variable ``embarked``
```{r}
titanic_test_strat %>%
 janitor::tabyl(embarked)
```

```{r}
titanic_train_strat %>%
 janitor::tabyl(embarked)
```


3. Variable ``pclass`` 
```{r}
titanic_test_strat %>%
 janitor::tabyl(pclass)
```

```{r}
titanic_train_strat %>%
 janitor::tabyl(pclass)
```

4. Variable ``sex``
```{r}
titanic_test_strat %>%
 janitor::tabyl(sex)
```

```{r}
titanic_train_strat %>%
 janitor::tabyl(sex)
```

5. Variable ``age_status``
```{r}
titanic_test_strat %>%
 janitor::tabyl(age_status)
```

```{r}
titanic_train_strat %>%
 janitor::tabyl(age_status)
```

Check to see how stratification affects ``parch`` distinct values
```{r}
titanic_test_strat %>%
 janitor::tabyl(parch)
```

```{r}
titanic_train_strat %>%
 janitor::tabyl(parch)
```


## Question 4      

<br> 
Create your decision tree to try and predict survival probability using an appropriate method, and create a decision tree plot.

**Ans**

Case#1 - no stratification
```{r}
titanic_fit <- rpart(
  formula = survived ~ ., 
  data = titanic_train, 
  method = 'class'
)

rpart.plot(titanic_fit, 
           yesno = 2, 
           fallen.leaves = TRUE, 
           faclen = 6, 
           digits = 3)
```

Case#2 - stratification
```{r}
titanic_fit_strat <- rpart(
  formula = survived ~ ., 
  data = titanic_train_strat, 
  method = 'class'
)

rpart.plot(titanic_fit_strat, 
           yesno = 2, 
           fallen.leaves = TRUE, 
           faclen = 6, 
           digits = 3)
```

## Question 5    

<br> 
Write down what this tells you, in detail. What variables are important? What does each node tell you? Who has the highest chance of surviving? Who has the lowest? Provide as much detail as you can.    

**Ans**
Interpretation of results according to stratified data:

+ The largest group in the data (42.6%) are lower & middle class adult male passengers with a probability of survival equal to 11.5% 

+ The 2nd largest group in the data (22.2%) are middle & upper class female passengers with a probability of survival equal to 93.7%

+ The 3rd largest group in the data (14.1) are upper class adult male passengers with a probability of survival equal to 39.5%

+ The 4th largest group in the data (7.9%) are lower class female passengers with 1 sibling/spouse aboard the Titanic with a probability of survival equal to 60.0%

+ The 5th largest group in the data (6.5%) are lower class female passengers with more than 1 sibling/spouse aboard the Titanic with a probability of survival equal to 29.7%

+ The 6th largest group in the data (3.0%) are lower & middle class under-aged male passengers with less than 2 siblings/spouses and more than one parent/children aboard with a probability of survival equal to 82.4%

+ The 7th largest group in the data (2.3%) are lower & middle class under-aged male passengers with 3 or more siblings/spouses  with a probability of survival equal to 0%

+ The smallest group in the data (1.6%) are lower & middle class under-aged male passengers that traveled with a nanny with a probability of survival equal to 11.1%

<br>

## Question 6     

<br>  
Test and add your predictions to your data. Create a confusion matrix. Write down in detail what this tells you for this specific dataset.  


**Ans**

Add predictions on test dataset:
```{r}
library(modelr)

# add the predictions
titanic_test_strat_pred <- titanic_test_strat %>%
  add_predictions(titanic_fit_strat, type = 'class')
```


```{r}
# look at the variables 
titanic_test_strat_pred %>%
  select(sex, pclass, age_status, sib_sp, parch, pred)
```

Create confusion matrix:
```{r}
library(yardstick)

conf_mat <- titanic_test_strat_pred %>%
              conf_mat(truth = survived, estimate = pred)

conf_mat
```

```{r}
tibble(
  accuracy = accuracy(titanic_test_strat_pred, truth = survived, estimate = pred)$.estimate,
  sensitivity = sensitivity(titanic_test_strat_pred, truth = survived, estimate = pred)$.estimate,
  specificity = specificity(titanic_test_strat_pred, truth = survived, estimate = pred)$.estimate
)
```


Alternative way: Calculate the confusion matrix and accuracy using ``caret`` package:
```{r}
library(caret)

confusionMatrix(titanic_test_strat_pred$pred, titanic_test_strat_pred$survived) #order is estimate and then truth 

```

# Extension  

See how a `ranger()` random forest classifier compares with a single decision tree in terms of performance. Can you tune the values of the `mtry`, `splitrule` and `min.node.size` hyperparameters? Which variables in the dataset turn out to be most important for your best model? The `Kappa` metric might be the best one to focus on if you want to improve performance for an imbalanced data set. Do some research on the definition of `Kappa` before you start.

We provide the code in the dropdown below if you get stuck, but still want to play around with this (note that run time can be up to 5-10 mins for the tuning). **Save your notebook before you begin** in case you need to force quit your session!


```{r, eval = TRUE}
library(ranger)

control <- trainControl(
  method = "repeatedcv", 
  number = 5, 
  repeats = 10
)

tune_grid = expand.grid(
  mtry = 1:6,
  splitrule = c("gini", "extratrees"),
  min.node.size = c(1, 3, 5)
)
```

```{r, eval = TRUE}
rf_tune <- train(
  survived ~ ., 
  data = titanic_train_strat, 
  method = "ranger",
  metric = "Kappa",
  num.trees = 1000,
  importance = "impurity",
  tuneGrid = tune_grid, 
  trControl = control
)

plot(rf_tune)
rf_tune
```

```{r}
rf_tune
```

Hyperparameters based on output of preceding code chunk
```{r}
rf_classifier <- ranger(survived ~ ., mtry = 2, data = titanic_train, splitrule = "gini", min.node.size = 5, num.trees = 1000, importance = "impurity")
```


Training set calcs for confusion matrix and accuracy:
```{r}
titanic_train_strat_pred <- titanic_train_strat %>%
  mutate(pred = predict(rf_classifier, data = titanic_train_strat)$predictions)

confusionMatrix(data = titanic_train_strat_pred$pred, reference = titanic_train_strat_pred$survived)
```

Test set calcs for confusion matrix and accuracy:
```{r}
titanic_test_strat_pred <- titanic_test_strat %>%
  mutate(pred = predict(rf_classifier, data = titanic_test_strat)$predictions)

confusionMatrix(data = titanic_test_strat_pred$pred, reference = titanic_test_strat_pred$survived)
```


Check measure of how important each variable is to the final performance of the classifier 
```{r}
importance(rf_classifier)
```

NB: I missed out the importance of ``sib_sp`` in MVP  Question 2 




